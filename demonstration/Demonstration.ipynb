{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e17662",
   "metadata": {},
   "source": [
    "# HERD Demo Notebook\n",
    "\n",
    "This notebook provides a step-by-step process to run the HERD prototype on your machine. Because HERD is still in its infancy, this notebook is currently limited in its functionality. This notebook lays the foundation for what HERD will look like as we move towards HERDS first official release, providing examples and information about how HERD is built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c07310",
   "metadata": {},
   "source": [
    "## Step 0: System Level Requirements\n",
    "\n",
    "HERD's underlying architecture heavily relies on Kubernetes to host experts. The easiest way to ensure Kubernetes is accessible on your system is to install Docker Desktop, navigate to your settings, and enable Kubernetes. Apart from this, all system-level requirements are handled using Docker-Compose files, so as long as you have Docker installed on your machine, you will be able to host and run HERD (given your machine has standard memory and compute available). HERD also relies on HELM. To install HELM run the following commands:\n",
    "\n",
    "How much memory/compute HERD uses is entirely up to you, meaning that memory and GPU/CPU access are not limiting factors. This demo will be geared to run on CPU with low-memory models for accessibility, but you can easily clone the repo and work with the Kubernetes chart to scale HERD across compute however you would like. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31f7cc",
   "metadata": {},
   "source": [
    "## Step 1: Starting HERD\n",
    "\n",
    "Unlike traditional MoE models, HERD instances exist as servers to enable dynamic insertion/deletion of experts and modular use of the Router and Aggregator. To run HERD on your local machine, you need to start up the HERD server using the Python cell located below. Starting up the HERD server will post API entry-points as well as setting up the k8s cluster for experts. For this demo, you can startup the server by simply navigating to the server directory and running `main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299b489",
   "metadata": {},
   "source": [
    "## Step 2: Loading up Experts\n",
    "\n",
    "The HERD server has an endpoint setup to load experts into tje Kubernetes cluster. When loading an expert you can set the name, model id, token limit, temperature, and port on the Kubernetes cluster. The python cell below gives you access to the API endpoint assuming your cluster is accessible through localhost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in an expert. \n",
    "\n",
    "import requests\n",
    "\n",
    "name = input(\"Enter expert name: \")\n",
    "model_id = input(\"Enter model ID (e.g., sshleifer/tiny-gpt2): \")\n",
    "max_new_tokens = int(input(\"Enter max new tokens (e.g., 50): \"))\n",
    "temperature = float(input(\"Enter temperature (e.g., 0.7): \"))\n",
    "node_port = int(input(\"Enter node port (e.g., 30088): \"))\n",
    "\n",
    "url = \"http://localhost:80/add_expert\"\n",
    "\n",
    "payload = {\n",
    "    \"name\": name,\n",
    "    \"model_id\": model_id,\n",
    "    \"max_new_tokens\": str(max_new_tokens),\n",
    "    \"temperature\": str(temperature),\n",
    "    \"node_port\": node_port\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
