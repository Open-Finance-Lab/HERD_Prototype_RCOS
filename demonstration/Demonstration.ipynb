{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e17662",
   "metadata": {},
   "source": [
    "# HERD Demo Notebook\n",
    "\n",
    "This notebook provides a step-by-step process to run the HERD prototype on your machine. Because HERD is still in its infancy, this notebook is currently limited in its functionality. This notebook lays the foundation for what HERD will look like as we move towards HERDS first official release, providing examples and information about how HERD is built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c07310",
   "metadata": {},
   "source": [
    "## Step 0: System Level Requirements\n",
    "\n",
    "HERD's underlying architecture heavily relies on Kubernetes to host experts. The easiest way to ensure Kubernetes is accessible on your system is to install Docker Desktop, navigate to your settings, and enable Kubernetes. Apart from this, all system-level requirements are handled using Docker-Compose files, so as long as you have Docker installed on your machine, you will be able to host and run HERD (given your machine has standard memory and compute available). HERD also relies on HELM. To install HELM run the following commands:\n",
    "\n",
    "How much memory/compute HERD uses is entirely up to you, meaning that memory and GPU/CPU access are not limiting factors. This demo will be geared to run on CPU with low-memory models for accessibility, but you can easily clone the repo and work with the Kubernetes chart to scale HERD across compute however you would like. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a31f7cc",
   "metadata": {},
   "source": [
    "## Step 1: Starting HERD\n",
    "\n",
    "Unlike traditional MoE models, HERD instances exist as servers to enable dynamic insertion/deletion of experts and modular use of the Router and Aggregator. To run HERD on your local machine, you need to start up the HERD server using the Python cell located below. Starting up the HERD server will post API entry-points as well as setting up the k8s cluster for experts. For this demo, you can startup the server by simply navigating to the server directory and running `main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299b489",
   "metadata": {},
   "source": [
    "## Step 2: Loading up Experts\n",
    "\n",
    "The HERD server has an endpoint setup to load experts into tje Kubernetes cluster. When loading an expert you can set the name, model id, token limit, temperature, and port on the Kubernetes cluster. The python cell below gives you access to the API endpoint assuming your cluster is accessible through localhost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in an expert. \n",
    "\n",
    "import requests\n",
    "\n",
    "name = input(\"Enter expert name: \")\n",
    "model_id = input(\"Enter model ID (e.g., sshleifer/tiny-gpt2): \")\n",
    "max_new_tokens = int(input(\"Enter max new tokens (e.g., 50): \"))\n",
    "temperature = float(input(\"Enter temperature (e.g., 0.7): \"))\n",
    "node_port = int(input(\"Enter node port (e.g., 30088): \"))\n",
    "\n",
    "url = \"http://localhost:80/add_expert\"\n",
    "\n",
    "payload = {\n",
    "    \"name\": name,\n",
    "    \"model_id\": model_id,\n",
    "    \"max_new_tokens\": str(max_new_tokens),\n",
    "    \"temperature\": str(temperature),\n",
    "    \"node_port\": node_port\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e3d77",
   "metadata": {},
   "source": [
    "## Optional Step: Hitting a specific expert\n",
    "\n",
    "For general health checks, or in the event that you only need the help of a very specific domain of knowledge, the following route can be used to hit a single expert based on their name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "expert_name = input(\"Enter expert name (e.g., expert1): \")\n",
    "namespace = input(\"Enter namespace (default = 'default'): \") or \"default\"\n",
    "prompt = input(\"Enter your prompt: \")\n",
    "max_new_tokens = int(input(\"Enter max new tokens (e.g., 50): \"))\n",
    "temperature = float(input(\"Enter temperature (e.g., 0.7): \"))\n",
    "top_p = float(input(\"Enter top_p (e.g., 0.95): \"))\n",
    "top_k = int(input(\"Enter top_k (e.g., 50): \"))\n",
    "repetition_penalty = float(input(\"Enter repetition penalty (e.g., 1.0): \"))\n",
    "\n",
    "url = f\"http://localhost:80/experts/{expert_name}/ask?namespace={namespace}\"\n",
    "\n",
    "payload = {\n",
    "    \"prompt\": prompt,\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"temperature\": temperature,\n",
    "    \"top_p\": top_p,\n",
    "    \"top_k\": top_k,\n",
    "    \"repetition_penalty\": repetition_penalty\n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "try:\n",
    "    print(\"Response:\", response.json())\n",
    "except Exception:\n",
    "    print(\"Raw Response:\", response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa232098",
   "metadata": {},
   "source": [
    "## Step 3: Routing a Query\n",
    "\n",
    "The block below executes the full HERD pipeline, taking in a query and outputting a final aggregated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16589b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "BASE_URL = \"http://localhost:80\"  \n",
    "NAMESPACE = \"default\"              \n",
    "\n",
    "CLASSIFY_TOP_K = 5                 \n",
    "SCORE_THRESHOLD = 0.20            \n",
    "\n",
    "\n",
    "TOPIC_TO_EXPERT: Dict[str, str] = {\n",
    "    \"Physics\": \"physics-expert\",\n",
    "    \"Math\": \"math-expert\",\n",
    "}\n",
    "\n",
    "\n",
    "GEN = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"temperature\": 0.4,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 50,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "def slugify_topic_to_expert(topic: str) -> str:\n",
    "    \"\"\"Fallback expert name: slugify the topic and append '-expert'.\"\"\"\n",
    "    slug = re.sub(r'[^a-zA-Z0-9]+', '-', topic).strip('-').lower()\n",
    "    return f\"{slug}-expert\"\n",
    "\n",
    "def pick_expert_name(topic: str) -> str:\n",
    "    return TOPIC_TO_EXPERT.get(topic, slugify_topic_to_expert(topic))\n",
    "\n",
    "def call_create_prompts(text: str, top_k: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    POST /create_prompts\n",
    "    Body: {\"text\": <query>, \"top_k\": <int>}\n",
    "    Returns: {\"original\": str, \"model\": str, \"topics\": [...], \"prompts\": {...}}\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/create_prompts\"\n",
    "    payload = {\"text\": text, \"top_k\": top_k}\n",
    "    r = requests.post(url, json=payload, headers={\"Content-Type\": \"application/json\"})\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def call_expert_ask(name: str, prompt: str, gen: Dict[str, Any], namespace: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    POST /experts/{name}/ask?namespace={namespace}\n",
    "    Body: {\"prompt\": \"...\", **GEN}\n",
    "    Returns the routed response payload from your ask_expert_by_name route.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/experts/{name}/ask\"\n",
    "    params = {\"namespace\": namespace}\n",
    "    body = {\"prompt\": prompt, **gen}\n",
    "    r = requests.post(url, params=params, json=body, timeout=180)\n",
    "    r.raise_for_status()\n",
    "    try:\n",
    "        return r.json()\n",
    "    except Exception:\n",
    "        return {\"status_code\": r.status_code, \"text\": r.text}\n",
    "\n",
    "def extract_text_from_model_response(resp: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Try to normalize common response shapes into plain text for aggregation.\n",
    "    Supports:\n",
    "      { \"response\": { \"completion\": \"...\"} }\n",
    "      { \"response\": { \"text\": \"...\"} }\n",
    "      or just dumps JSON as fallback\n",
    "    \"\"\"\n",
    "    if isinstance(resp, dict) and \"response\" in resp and isinstance(resp[\"response\"], dict):\n",
    "        inner = resp[\"response\"]\n",
    "        return inner.get(\"completion\") or inner.get(\"text\") or json.dumps(inner, ensure_ascii=False)\n",
    "    return json.dumps(resp, ensure_ascii=False)\n",
    "\n",
    "def run_pipeline(user_query: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Create prompts (classification + specialization)\n",
    "    2) Select experts (score â‰¥ threshold)\n",
    "    3) Query each selected expert via /experts/{name}/ask\n",
    "    4) Aggregate outputs\n",
    "    \"\"\"\n",
    "    cp = call_create_prompts(user_query, CLASSIFY_TOP_K)\n",
    "    topics = cp.get(\"topics\", [])  # each: {\"topic\": \"...\", \"score\": \".../number\"}\n",
    "    prompts_by_topic = {k: v.get(\"prompt\") for k, v in cp.get(\"prompts\", {}).items()}\n",
    "\n",
    "    selected = [t for t in topics if float(t.get(\"score\", 0.0)) >= SCORE_THRESHOLD]\n",
    "\n",
    "    per_expert: List[Dict[str, Any]] = []\n",
    "    for t in selected:\n",
    "        topic = t[\"topic\"]\n",
    "        expert_name = pick_expert_name(topic)\n",
    "        # Prefer specialized prompt; fallback to original query\n",
    "        prompt_to_send = prompts_by_topic.get(topic, user_query)\n",
    "\n",
    "        resp = call_expert_ask(expert_name, prompt_to_send, GEN, NAMESPACE)\n",
    "        text = extract_text_from_model_response(resp)\n",
    "\n",
    "        per_expert.append({\n",
    "            \"topic\": topic,\n",
    "            \"score\": float(t.get(\"score\", 0.0)),\n",
    "            \"expert\": expert_name,\n",
    "            \"used_specialized_prompt\": topic in prompts_by_topic,\n",
    "            \"prompt_sent\": prompt_to_send,\n",
    "            \"raw_response\": resp,\n",
    "            \"text\": text,\n",
    "        })\n",
    "\n",
    "    per_expert_sorted = sorted(per_expert, key=lambda x: x[\"score\"], reverse=True)\n",
    "    combined_answer = \"\\n\\n\".join(\n",
    "        [f\"[{e['expert']} â€¢ {e['topic']} â€¢ score={e['score']:.2f}]\\n{e['text']}\" for e in per_expert_sorted]\n",
    "    ) if per_expert_sorted else \"(no experts selected)\"\n",
    "\n",
    "    return {\n",
    "        \"query\": user_query,\n",
    "        \"topics\": topics,\n",
    "        \"selected\": [{\"topic\": e[\"topic\"], \"score\": e[\"score\"], \"expert\": e[\"expert\"]} for e in per_expert_sorted],\n",
    "        \"answers\": per_expert_sorted,\n",
    "        \"combined_answer\": combined_answer,\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = input(\"Enter your prompt: \").strip()\n",
    "    out = run_pipeline(q)\n",
    "\n",
    "    print(\"\\n== Selected experts ==\")\n",
    "    if not out[\"selected\"]:\n",
    "        print(\"None (no topic met the threshold).\")\n",
    "    else:\n",
    "        for s in out[\"selected\"]:\n",
    "            print(f\" - {s['expert']} (topic={s['topic']}, score={s['score']:.2f})\")\n",
    "\n",
    "    print(\"\\n== Combined Answer ==\")\n",
    "    print(out[\"combined_answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
