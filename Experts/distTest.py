import os
import torch
import torch.distributed as dist
import time
import socket
import threading
import sys
from datetime import timedelta


# ======= Timeout Safeguard =======
def kill_after_timeout(timeout_seconds: int):
    def monitor():
        print(f"[{socket.gethostname()}] ğŸ• Watchdog started. Will kill process in {timeout_seconds} seconds.")
        time.sleep(timeout_seconds)
        print(f"[{socket.gethostname()}] â›” Timeout reached. Exiting.")
        sys.exit(1)
    t = threading.Thread(target=monitor, daemon=True)
    t.start()


# ======= Debug Environment =======
def debug_env():
    print(f"[{socket.gethostname()}] ğŸŒ ENV CHECKPOINT")
    print(f"[{socket.gethostname()}] ENV RANK         = {os.environ.get('RANK')}")
    print(f"[{socket.gethostname()}] ENV WORLD_SIZE   = {os.environ.get('WORLD_SIZE')}")
    print(f"[{socket.gethostname()}] ENV MASTER_ADDR  = {os.environ.get('MASTER_ADDR')}")
    print(f"[{socket.gethostname()}] ENV MASTER_PORT  = {os.environ.get('MASTER_PORT')}")


# ======= Safe Init Process Group =======
def safe_init_process_group(backend, init_method, rank, world_size):
    try:
        print(f"[{socket.gethostname()}][Rank {rank}] ğŸ”„ Starting init_process_group with {backend}...")
        dist.init_process_group(
            backend=backend,
            init_method=init_method,
            rank=rank,
            world_size=world_size,
            timeout=timedelta(seconds=30)
        )
        print(f"[{socket.gethostname()}][Rank {rank}] âœ… Process group initialized.")
    except Exception as e:
        print(f"[{socket.gethostname()}][Rank {rank}] âŒ init_process_group failed: {e}")
        sys.exit(1)


# ======= Main Entrypoint =======
def main():
    print(f"[{socket.gethostname()}] ğŸš€ Script has started.")
    kill_after_timeout(timeout_seconds=1200)

    debug_env()

    try:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        master_addr = os.environ['MASTER_ADDR']
        master_port = os.environ['MASTER_PORT']
    except KeyError as e:
        print(f"[{socket.gethostname()}] âŒ Missing environment variable: {e}")
        sys.exit(1)

    print(f"[{socket.gethostname()}][Rank {rank}] ğŸ“¡ All environment variables loaded.")

    init_method = f"tcp://{master_addr}:{master_port}"
    backend = "nccl" if torch.cuda.is_available() else "gloo"
    print(f"[{socket.gethostname()}][Rank {rank}] âš™ï¸  Backend selected: {backend}")
    print(f"[{socket.gethostname()}][Rank {rank}] ğŸ”Œ Init method: {init_method}")

    safe_init_process_group(backend, init_method, rank, world_size)

    print(f"[{socket.gethostname()}][Rank {rank}] ğŸ§® Allocating tensor...")
    tensor = torch.zeros(1).cuda() if torch.cuda.is_available() else torch.zeros(1)
    print(f"[{socket.gethostname()}][Rank {rank}] ğŸŸ© Tensor initialized with value {tensor.item()}")

    if rank == 0:
        print(f"[{socket.gethostname()}][Rank 0] ğŸ“ Adding 42 to tensor...")
        tensor += 42
        print(f"[{socket.gethostname()}][Rank 0] ğŸ“¤ Sending tensor to rank 1...")
        dist.send(tensor=tensor, dst=1)
        print(f"[{socket.gethostname()}][Rank 0] âœ… Tensor sent.")
    elif rank == 1:
        print(f"[{socket.gethostname()}][Rank 1] ğŸ“¥ Waiting to receive tensor from rank 0...")
        dist.recv(tensor=tensor, src=0)
        print(f"[{socket.gethostname()}][Rank 1] ğŸ“¦ Received tensor with value: {tensor.item()}")

    print(f"[{socket.gethostname()}][Rank {rank}] ğŸ§¹ Destroying process group...")
    dist.destroy_process_group()
    print(f"[{socket.gethostname()}][Rank {rank}] ğŸ Exiting cleanly.")


if __name__ == "__main__":
    main()
